# ğŸš€ Real-Time Streaming Analytics <br>(Kafka â†’ Spark â†’ Postgres â†’ FastAPI)

A Docker Compose-based real-time analytics pipeline:
- **Kafka** for event ingestion
- **Spark Structured Streaming** for minute-window aggregations
- **Postgres** for durable metrics storage (upsert-safe)
- **FastAPI** for query APIs (`/metrics`, `/streams/top`, etc.)
- **Smoke tests** to validate the full pipeline end-to-end

This repo is designed to be:
- **one-command runnable**
- **CI-friendly**
- **restart-safe**

---

## ğŸ§± Architecture

```mermaid
---
config:
  layout: elk
  theme: mc
---
flowchart LR
 subgraph subGraph0["Docker Compose"]
        EG["Event Generator"]
        K[("Kafka")]
        SP["Spark Structured Streaming"]
        PG[("Postgres")]
        API["FastAPI Metrics API"]
  end
    EG -- JSON events --> K
    K -- readStream --> SP
    SP -- UPSERT per minute window --> PG
    API -- SQL queries --> PG
```

---
## ğŸ“š Documentation

- ğŸ“˜ **[Runbook](docs/runbook.md)** â€” how to run, debug, and recover the system  
- ğŸ§  **[Architecture Decisions](docs/decisions.md)** â€” why these tools and patterns were chosen  
- ğŸ› ï¸ **[Scripts Reference](scripts/README.md)** â€” helper scripts explained  
- ğŸ—‚ï¸ **[Project Index](Index.md)** â€” navigation + notes

---

## Repo Map

### Top-level
- `docker-compose.yml` â€” full stack definition
- `Makefile` â€” shortcuts (`make up`, `make reset`, `make smoke`, `make commit`)
- `README.md` â€” project overview
- `Index.md` â€” project notes / index
- `docs/`
  - `runbook.md` â€” operational guide
  - `decisions.md` â€” architectural decisions
- `architecture/`
  - `architecture.mmd` â€” diagram source
  - `docker.png` â€” exported architecture diagram

### Services
- `services/event-generator/` â€” Kafka producer
- `services/stream-processor/` â€” Spark Structured Streaming job
- `services/metrics-api/` â€” FastAPI metrics service

### Database
- `sql/init/001_stream_tables.sql` â€” Postgres init schema
- `sql/postgres_init.sql` â€” legacy init (optional)

### Scripts
- `scripts/start.sh` â€” start stack
- `scripts/stop.sh` â€” stop stack
- `scripts/reset.sh` â€” full reset (wipe volumes)
- `scripts/smoke.sh` â€” end-to-end smoke test
- `scripts/doctor.sh` â€” diagnostics
- `scripts/git_commit.sh` â€” commit helper
- `scripts/lib.sh` â€” shared bash helpers
---
## â–¶ï¸ Quickstart

```bash
make up
make smoke
```

â™»ï¸ Full reset (wipe volumes, re-run DB init):
```bash
make reset
```

---

## ğŸ§ª Smoke Tests

Smoke tests validate:
- ğŸ“¡ Kafka topic exists
- âš¡ Producer offsets advance
- ğŸ”¥ Spark is healthy
- ğŸ˜ Postgres has recent rows
- ğŸ’° Donations aggregate correctly
- ğŸŒ API endpoints respond

```bash
make smoke
```

---

## ğŸŒ API Examples

```bash
curl -s http://localhost:8000/health
curl -s "http://localhost:8000/metrics?minutes=10&limit=5" | jq
curl -s "http://localhost:8000/streams/top?minutes=10&by=donations_usd&n=5" | jq
```

---

## Database Notes

Postgres init scripts run only on a fresh data directory.

If tables did not change:
```bash
make reset
```

---

## Troubleshooting

Run diagnostics:
```bash
bash scripts/doctor.sh
```

Check logs:
```bash
docker compose logs -f spark-stream
```

---

## Git Workflow

Auto commit:
```bash
make commit
```

Custom message:
```bash
make commit MSG="api: improve top streams endpoint"
```

---

## ğŸ“Š Observability (Planned)

- ğŸ“ˆ Prometheus metrics
- ğŸ“Š Grafana dashboards

---

## Roadmap

- [ ] Observability
- [ ] Dashboard screenshots
- [ ] Advanced API filters
